import std:array
import std:dict
import std:dict:Dict
import std:list
import std:math
import std:order
import std:result
import std:result:Result
import std:string

import ritec:diagnostic
import ritec:diagnostic:Diagnostic
import ritec:diagnostic:report
import ritec:diagnostic:report:Report
import ritec:span:Span

import token:Base
import token:Delimiter
import token:Doc
import token:Keyword
import token:Punct
import token:TokenStream
import token:TokenTree

/// Lex a source file.
pub fn lex(source: int, text: str) -> Result<TokenStream, Report>
  let lexer = new(source, 0, text)
  match lex_tokens(lexer, [])
  | Ok((tokens, lexer)) ->
    let span = Span(
      lo:     lexer.start
      hi:     lexer.offset
      source: source
    )

    Ok(TokenStream(tokens, span)) 
  | Err(e) ->
    report:new()
    |> report:add(e)
    |> Err

type Lexer(
  source:     int
  start:      int
  offset:     int
  graphemes:  [str]
  digits:     [str]
  keywords:   Dict<str, Keyword>
  puncts:     Dict<str, Punct>
)

fn new(source, offset, text)
  Lexer(
    source:     source
    start:      offset
    offset:     offset
    graphemes:  string:graphemes(text)
    digits:     token:digits()
    keywords:   token:keywords()
    puncts:     token:puncts()
  )

fn peek(lexer)
  peek_nth(lexer, 0)

fn peek_nth(lexer: Lexer, n)
  list:nth(lexer.graphemes, n)

fn next(lexer: Lexer)
  let grapheme = peek(lexer)?
  let lexer = advance(lexer, 1)
  Ok((grapheme, lexer))

fn advance(lexer: Lexer, n: int)
  let offset = list:take(lexer.graphemes, n), lexer.offset
    |> list:fold(|g, offset| offset + string:length(g))

  Lexer(
    offset: offset
    graphemes: list:drop(lexer.graphemes, n)
    ..lexer
  )

type Indent =
  | Tabs(n: int)
  | Spaces(n: int)

fn is_newline(g)
  g == string:from_bytes(array:from_list([10]))

fn lex_tokens(
  lexer: Lexer
  indents: [Indent]
) -> Result<[TokenTree * Span] * Lexer, Diagnostic>  
  match peek(lexer)
  | Err(_) -> Ok(([], lexer))
  | Ok(g) ->
    match is_newline(g)
    | true ->
      let span = Span(
        lo:     lexer.offset
        hi:     lexer.offset + 1
        source: lexer.source
      )

      let lexer = advance(lexer, 1)

      match skip_empty_line(lexer)
      | Ok(lexer) -> 
        let tokens, lexer = lex_tokens(lexer, indents)?
        Ok(([(Punct(Newline), span), ..tokens], lexer))
      | Err(_) ->
        let new_indents, lexer = lex_indents(lexer, indents)?

        match math:cmp(list:length(new_indents), list:length(indents))
        | order:Lt -> Ok(([], lexer))
        | order:Eq -> 
          let tokens, lexer = lex_tokens(lexer, indents)?

          Ok(([(Punct(Newline), span), ..tokens], lexer))
        | order:Gt ->
          let tokens, new_lexer = lex_tokens(lexer, new_indents)?

          let span = Span(
            lo:     lexer.offset
            hi:     new_lexer.offset
            source: lexer.source
          )

          let stream = TokenStream(tokens, span)
          let group = token:Group:Group(token:Indent, stream)
          let token = token:TokenTree:Group(group)

          let tokens, lexer = lex_tokens(new_lexer, indents)?
          Ok(([(token, span), ..tokens], lexer))
    | false ->
      match string:is_whitespace(g)
      | true -> 
        let lexer = skip_whitespace(lexer)
        lex_tokens(lexer, indents)
      | false ->
        match peek(lexer) == Ok("/") && peek_nth(lexer, 1) == Ok("/")
        | true ->
          let lexer = advance(lexer, 2)

          let level, lexer = match peek(lexer) == Ok("/")
            | true -> Ok(Item), advance(lexer, 1)
            | false ->
              match peek(lexer) == Ok("!")
              | true -> Ok(Module), advance(lexer, 1)
              | false -> Err(void), lexer

          let comment, new_lexer = lex_comment(lexer)

          match level
          | Ok(level) ->
            let span = Span(
              lo:     lexer.offset
              hi:     new_lexer.offset
              source: lexer.source
            )
            
            let token = token:TokenTree:Doc(level, comment)
            let tokens, lexer = lex_tokens(new_lexer, indents)?
            Ok(([(token, span), ..tokens], lexer))
          | Err(_) -> lex_tokens(new_lexer, indents)
        | false ->
          match lex_open_delim(lexer) 
          | Ok((delimiter, lexer)) ->
            let tokens, new_lexer = lex_tokens(lexer, indents)?

            let span = Span(
              lo:     lexer.offset
              hi:     new_lexer.offset
              source: lexer.source
            )

            let stream = TokenStream(tokens, span)
            let group = token:Group:Group(delimiter, stream)
            let token = token:TokenTree:Group(group)

            let tokens, lexer = lex_tokens(new_lexer, indents)?
            Ok(([(token, span), ..tokens], lexer))
          | Err(_) ->
            match lex_close_delim(lexer)
            | Ok((delimiter, lexer)) -> Ok(([], lexer))
            | Err(_) ->
              let token, span, lexer = lex_token(lexer, indents, g)?
              let tokens, lexer = lex_tokens(lexer, indents)?
              Ok(([(token, span), ..tokens], lexer)) 

fn lex_token(
  lexer: Lexer
  indents: [Indent]
  g: str
) -> Result<TokenTree * Span * Lexer, Diagnostic>
  let quote = string:from_bytes(array:from_list([34]))
  
  match g == quote
  | true ->
    let lexer = advance(lexer, 1)
    let string, new_lexer = lex_string(lexer)

    let span = Span(
      lo:     lexer.offset
      hi:     new_lexer.offset
      source: lexer.source
    )

    let token = String(string)
    Ok((token, span, new_lexer))
  | false ->
    match g == "f" && peek_nth(lexer, 1) == Ok(quote)
    | true ->
      let lexer = advance(lexer, 2)
      let string, tokens, new_lexer = lex_format(lexer, indents)?

      let span = Span(
        lo:     lexer.offset
        hi:     new_lexer.offset
        source: lexer.source
      )

      let token = Format(string, tokens)
      Ok((token, span, new_lexer))
    | false ->
      match is_ident_start(g)
      | true ->
        let ident, new_lexer = lex_ident(lexer)

        match lex_path(new_lexer)
        | Ok((segments, new_lexer)) ->
          let span = Span(
            lo:     lexer.offset
            hi:     new_lexer.offset
            source: lexer.source
          )

          let token = Path([ident, ..segments])
          Ok((token, span, new_lexer)) 
        | Err(_) ->  
          let span = Span(
            lo:     lexer.offset
            hi:     new_lexer.offset
            source: lexer.source
          )

          let token = match dict:get(lexer.keywords, ident)
            | Ok(keyword) -> Keyword(keyword)
            | Err(_) ->
              match string:is_uppercase(g)
              | true -> Pascal(ident)
              | false -> Snake(ident)

          Ok((token, span, new_lexer))
      | false ->
        match string:is_numeric(g)
        | true ->
          let base, integer, new_lexer = lex_integer(lexer)

          let span = Span(
            lo:     lexer.offset
            hi:     new_lexer.offset
            source: lexer.source
          )

          let token = Integer(base, integer)
          Ok((token, span, new_lexer))
        | false ->
          match lex_punct(lexer) 
          | Ok(token) -> Ok(token)
          | Err(_) ->
            let span = Span(
              lo:     lexer.offset
              hi:     lexer.offset + 1
              source: lexer.source
            )

            diagnostic:new(
              severity:  diagnostic:Error
              code:      "E0000"
              message:   "unexpected character"
            )
            |> diagnostic:add_label("here", span)
            |> Err

fn lex_comment(lexer: Lexer)
  match peek(lexer) 
  | Err(_) -> "", lexer
  | Ok(g) ->
    match is_newline(g)
    | true -> "", lexer
    | false ->
      let lexer = advance(lexer, 1)
      let comment, lexer = lex_comment(lexer)
      string:concat(g, comment), lexer

fn lex_open_delim(lexer: Lexer)
  match peek(lexer) == Ok("(")
  | true -> Ok((Parentheses, advance(lexer, 1)))
  | false ->
    match peek(lexer) == Ok("[")
    | true -> Ok((Brackets, advance(lexer, 1)))
    | false ->
      match peek(lexer) == Ok("{")
      | true -> Ok((Braces, advance(lexer, 1)))
      | false -> Err(void)

fn lex_close_delim(lexer: Lexer)
  match peek(lexer) == Ok(")")
  | true -> Ok((Parentheses, advance(lexer, 1)))
  | false ->
    match peek(lexer) == Ok("]")
    | true -> Ok((Brackets, advance(lexer, 1)))
    | false ->
      match peek(lexer) == Ok("}")
      | true -> Ok((Braces, advance(lexer, 1)))
      | false -> Err(void)

fn lex_punct(lexer) -> Result<TokenTree * Span * Lexer, void>
  match lex_punct2(lexer), lex_punct1(lexer)
  | Ok((punct, new_lexer)), _ ->
    let span = Span(
      lo:     lexer.offset
      hi:     new_lexer.offset
      source: lexer.source
    )

    let token = Punct(punct)
    Ok((token, span, new_lexer))
  | _, Ok((punct, new_lexer)) ->
    let span = Span(
      lo:     lexer.offset
      hi:     new_lexer.offset
      source: lexer.source
    )

    let token = Punct(punct)
    Ok((token, span, new_lexer))
  | _, _ -> Err(void)

fn lex_punct2(lexer: Lexer)
  match peek(lexer), peek_nth(lexer, 1)
  | Ok(g1), Ok(g2) ->
    match dict:get(lexer.puncts, string:concat(g1, g2))
    | Ok(punct) -> Ok((punct, advance(lexer, 2)))
    | Err(_) -> Err(void)
  | _ -> Err(void)

fn lex_punct1(lexer: Lexer)
  match peek(lexer)
  | Err(_) -> Err(void)
  | Ok(g) ->
    match dict:get(lexer.puncts, g)
    | Ok(punct) -> Ok((punct, advance(lexer, 1)))
    | Err(_) -> Err(void)

fn is_ident_start(g)
  string:is_alphabetic(g) || g == "_"

fn lex_ident(lexer: Lexer)
  match peek(lexer)
  | Err(_) -> "", lexer
  | Ok(g) ->
    match string:is_alphanumeric(g) || g == "_"
    | false -> "", lexer
    | true ->
      let lexer = advance(lexer, 1)
      let ident, lexer = lex_ident(lexer)
      string:concat(g, ident), lexer

fn lex_path(lexer: Lexer) -> Result<[str] * Lexer, void>
  match peek(lexer) == Ok(":"), result:map(peek_nth(lexer, 1), is_ident_start)
  | true, Ok(true) ->
    let lexer = advance(lexer, 1)
    let ident, lexer = lex_ident(lexer)

    match lex_path(lexer)
    | Ok((segments, lexer)) -> Ok(([ident, ..segments], lexer))
    | Err(_) -> Ok(([ident], lexer))
  | _ -> Err(void)

fn lex_string(lexer: Lexer)
  let quote = string:from_bytes(array:from_list([34]))

  match peek(lexer)
  | Err(_) -> "", lexer
  | Ok(g) ->
    match g == quote
    | true -> "", advance(lexer, 1)
    | false ->
      let lexer = advance(lexer, 1)
      let string, lexer = lex_string(lexer)
      string:concat(g, string), lexer

fn lex_format(lexer, indents) -> Result<str * [TokenStream * str] * Lexer, Diagnostic>
  let quote = string:from_bytes(array:from_list([34]))

  match peek(lexer) 
  | Err(_) ->
    let span = Span(
      lo:     lexer.offset
      hi:     lexer.offset
      source: lexer.source
    )

    diagnostic:new(
      severity:  diagnostic:Error
      code:      "E0003"
      message:   "unexpected end of file"
    )
    |> diagnostic:add_label("here", span)
    |> Err
  | Ok(g) ->
    match g == quote
    | true -> Ok(("", [], advance(lexer, 1)))
    | false ->
      match g == "{"
      | true ->
        let lexer = advance(lexer, 1)
        let tokens, new_lexer = lex_tokens(lexer, indents)?

        let span = Span(
          lo:     lexer.offset
          hi:     new_lexer.offset
          source: lexer.source
        )

        let token = TokenStream(tokens, span)

        let string, tokens, lexer = lex_format(new_lexer, indents)?
        Ok(("", [(token, string), ..tokens], lexer))
      | false ->
        let lexer = advance(lexer, 1)
        let string, tokens, lexer = lex_format(lexer, indents)?
        Ok((string:concat(g, string), tokens, lexer))

fn lex_integer(lexer: Lexer) -> Base * int * Lexer
  let base, lexer = 
    match peek(lexer) == Ok("0") && peek_nth(lexer, 1) == Ok("b")
    | true -> Bin, advance(lexer, 2)
    | false ->
      match peek(lexer) == Ok("0") && peek_nth(lexer, 1) == Ok("o")
      | true -> Oct, advance(lexer, 2)
      | false ->
        match peek(lexer) == Ok("0") && peek_nth(lexer, 1) == Ok("x")
        | true -> Hex, advance(lexer, 2)
        | false -> Dec, lexer

  let radix = match base
    | Bin -> 2
    | Oct -> 8
    | Dec -> 10
    | Hex -> 16

  let integer, lexer = lex_integer_rec(lexer, radix, 0)
  base, integer, lexer

fn lex_integer_rec(lexer: Lexer, radix, integer)
  match peek(lexer)
  | Err(_) -> integer, lexer
  | Ok(g) ->
    match list:position(lexer.digits, |d| d == g)
    | Err(_) -> integer, lexer
    | Ok(n) ->
      let lexer = advance(lexer, 1)
      let integer = integer * radix + n
      lex_integer_rec(lexer, radix, integer)

fn lex_indents(lexer: Lexer, current: [Indent])
  mut lexer = lexer

  // we generate a tab character because they cannot currently
  // be expressed in a string literal
  let tab = string:from_bytes(array:from_list([9]))

  let result = list:reverse(current), []
    |> list:try_fold(|indent, indents|
      match peek(lexer) == Ok(tab) || peek(lexer) == Ok(" ")
      | false -> Err(Ok(indents))
      | true ->
        match indent
        | Tabs(n) ->
          lexer = result:map_err(expect_indent(lexer, tab, n), Err)?
          Ok([Tabs(n), ..indents])
        | Spaces(n) ->
          lexer = result:map_err(expect_indent(lexer, " ", n), Err)?
          Ok([Spaces(n), ..indents])
    )

  match result
  | Err(Err(e)) -> Err(e)
  | Err(Ok(indents)) -> Ok((indents, lexer))
  | Ok(indents) ->
    match lex_indent(lexer)?
    | Err(_) -> Ok((indents, lexer))
    | Ok((indent, lexer)) -> 
      let indents = [indent, ..indents]
      Ok((indents, lexer))


fn lex_indent(lexer: Lexer)
  // if there are no more graphemes, there can be no indent
  match peek(lexer) 
  | Err(_) -> Ok(Err(void))
  | Ok(first) ->
    // we generate a tab character because they cannot currently
    // be expressed in a string literal
    let tab = string:from_bytes(array:from_list([9]))

    // if the first grapheme is a tab or space, then we have an indent
    match first == tab || first == " "
    | false -> Ok(Err(void))
    | true ->
      // we need to compute the number of graphemes in the indent
      let result = lexer.graphemes, 0
        |> list:try_fold(|g, acc|
          // if the indent contains a mix of tabs and spaces, we
          // report an error
          match (g == tab || g == " ") && g != first
          | true ->
            let span = Span(
              lo:     lexer.offset
              hi:     lexer.offset + acc
              source: lexer.source
            )

            diagnostic:new(
              severity:  diagnostic:Error
              code:      "E0001"
              message:   "indentation must be consistent, and cannot mix tabs and spaces"
            )
            |> diagnostic:add_label("here", span)
            |> Err
            |> Err
          | false ->
            // otherwise check if the grapheme is a tab or space
            // and increment the count
            match g == tab || g == " "
            | true -> Ok(acc + 1)
            | false -> Err(Ok(acc))
        )

      // return the error if there was one
      let n = match result
        | Err(Err(e)) -> Err(e)
        | Err(Ok(n)) -> Ok(n)
        | Ok(n) -> Ok(n)
      let n = n?

      match first == tab
      | true -> Ok(Ok((Tabs(n), advance(lexer, n))))
      | false -> Ok(Ok((Spaces(n), advance(lexer, n))))

fn expect_indent(lexer: Lexer, g, n)
  match list:first(lexer.graphemes) == Ok(g)
  | true -> Ok(advance(lexer, n))
  | false ->
    let span = Span(
      lo:     lexer.offset
      hi:     lexer.offset + n
      source: lexer.source
    )

    diagnostic:new(
      severity:  diagnostic:Error
      code:      "E0002"
      message:   "expected a indentation"
    )
    |> diagnostic:add_label("here", span)
    |> Err

/// Skip an empty line.
fn skip_empty_line(lexer: Lexer)
  let newline = string:from_bytes(array:from_list([10]))

  lexer.graphemes, lexer
  |> list:try_fold(|g, lexer|
    match string:is_whitespace(g)
    | false -> Err(Err(void))
    | true ->
      match g == newline
      | false -> Ok(advance(lexer, 1))
      | true -> Err(Ok(lexer))
  )
  |> result:flatten_err

/// Skip whitespace characters.
fn skip_whitespace(lexer: Lexer)
  lexer.graphemes, lexer
  |> list:try_fold(|g, lexer|
    match string:is_whitespace(g) && !is_newline(g)
    | true -> Ok(advance(lexer, 1))
    | false -> Err(lexer)
  )
  |> result:unwrap
