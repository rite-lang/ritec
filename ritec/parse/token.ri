import std:array
import std:dict
import std:list
import std:string
import std:result:Result

import ritec:span:Span
import ritec:diagnostic
import ritec:diagnostic:Diagnostic
import ritec:diagnostic:Severity

/// A token stream.
///
/// The primary data structure used by the parser.
pub type TokenStream(
  tokens: [TokenTree * Span]
  span: Span
)

/// A base for integer literals.
pub type Base =
  /// A binary integer literal.
  | Bin

  /// An octal integer literal.
  | Oct

  /// A decimal integer literal.
  | Dec

  /// A hexadecimal integer literal.
  | Hex

pub type TokenTree =
  | Eof
  | Doc(level: Doc, comment: str)
  | Snake(snake: str)
  | Pascal(pascal: str)
  | Path(path: [str])
  | String(string: str)
  | Format(start: str, tokens: [TokenStream * str])
  | Integer(base: Base, integer: int)
  | Punct(punct: Punct)
  | Keyword(keyword: Keyword)
  | Group(delimiter: Delimiter, tokens: TokenStream)

pub type Doc =
  | Module
  | Item

pub type Delimiter =
  | Parentheses
  | Brackets
  | Braces
  | Indent

pub type Keyword =
  | As
  | Assert
  | Bool
  | F32
  | F64
  | False
  | Fn
  | I16
  | I32
  | I64
  | I8
  | If
  | Import
  | Int
  | Let
  | Match
  | Mut
  | Panic
  | Pub
  | Pure
  | Return
  | Str
  | Todo
  | True
  | Type
  | U16
  | U32
  | U64
  | U8
  | Unreachable
  | Void

pub type Punct =
  | Newline

  // Double-character punctuations
  | DotDot
  | EqEq
  | NotEq
  | LtEq
  | GtEq
  | AndAnd
  | OrOr
  | OrGt
  | Arrow

  // Single-character punctuations
  | Colon
  | Semi
  | Dot
  | Comma
  | Eq
  | Not
  | And
  | Or
  | Lt
  | Gt
  | Plus
  | Minus
  | Star
  | Slash
  | Percent
  | Quote
  | Under
  | Pound
  | Question

pub fn format(token: TokenTree) -> str
  match token
  | Eof -> "EOF"
  | Doc(level, comment) -> comment
  | Snake(snake) -> snake
  | Pascal(pascal) -> pascal
  | Path(path) -> string:join(path, ":")
  | String(string) -> string
  | Format(start, tokens) -> "{format}"
  | Integer(_, _) -> "{integer}"
  | Punct(punct) -> format_punct(punct)
  | Keyword(keyword) -> format_keyword(keyword)
  | Group(delimiter, _) ->
    match delimiter
    | Parentheses -> "(...)"
    | Brackets    -> "[...]"
    | Braces      -> "{...}"
    | Indent      -> "indent"

pub fn format_keyword(keyword: Keyword) -> str
  match keyword
  | As -> "as"
  | Assert -> "assert"
  | Bool -> "bool"
  | F32 -> "f32"
  | F64 -> "f64"
  | False -> "false"
  | Fn -> "fn"
  | I16 -> "i16"
  | I32 -> "i32"
  | I64 -> "i64"
  | I8 -> "i8"
  | If -> "if"
  | Import -> "import"
  | Int -> "int"
  | Let -> "let"
  | Match -> "match"
  | Mut -> "mut"
  | Panic -> "panic"
  | Pub -> "pub"
  | Pure -> "pure"
  | Return -> "return"
  | Str -> "str"
  | Todo -> "todo"
  | True -> "true"
  | Type -> "type"
  | U16 -> "u16"
  | U32 -> "u32"
  | U64 -> "u64"
  | U8 -> "u8"
  | Unreachable -> "unreachable"
  | Void -> "void"

pub fn format_punct(punct: Punct) -> str
  match punct
  | Newline -> "newline"
  | DotDot -> ".."
  | EqEq -> "=="
  | NotEq -> "!="
  | LtEq -> "<="
  | GtEq -> ">="
  | AndAnd -> "&&"
  | OrOr -> "||"
  | OrGt -> "|>"
  | Arrow -> "->"
  | Colon -> ":"
  | Semi -> ";"
  | Dot -> "."
  | Comma -> ","
  | Eq -> "="
  | Not -> "!"
  | And -> "&"
  | Or -> "|"
  | Lt -> "<"
  | Gt -> ">"
  | Plus -> "+"
  | Minus -> "-"
  | Star -> "*"
  | Slash -> "/"
  | Percent -> "%"
  | Quote -> "'"
  | Under -> "_"
  | Pound -> "#"
  | Question -> "?"

pub fn is_eof(stream: TokenStream) -> bool
  list:is_empty(stream.tokens)

pub fn peek(stream: TokenStream) -> TokenTree * Span
  peek_nth(stream, 0)

pub fn peek_nth(stream: TokenStream, n: int) -> TokenTree * Span
  match list:nth(stream.tokens, n)
  | Ok(token) -> token
  | Err(_) ->
    let span = Span(
      lo:     stream.span.hi
      hi:     stream.span.hi
      source: stream.span.source
    )

    Eof, span

pub fn next(stream: TokenStream) -> TokenTree * Span * TokenStream
  let token, span = peek(stream)
  token, span, advance(stream, 1)

pub fn advance(stream: TokenStream, n: int) -> TokenStream
  let tokens = list:drop(stream.tokens, n)
  TokenStream(tokens: tokens, ..stream)

pub fn is(stream: TokenStream, expected: TokenTree) -> bool
  nth_is(stream, 0, expected) 

pub fn nth_is(stream: TokenStream, n: int, expected: TokenTree) -> bool
  let token, _ = peek_nth(stream, n)
  token == expected 

pub fn take(
  stream: TokenStream
  expected: TokenTree
) -> Result<TokenTree * Span, void> * TokenStream
  let token, span = peek(stream)
  match token == expected
  | true  -> Ok((token, span)), advance(stream, 1)
  | false -> Err(void), stream

/// Keep taking tokens (and drop them) until we no longer see it
/// for instance newlines.
pub fn take_all(
  stream: TokenStream
  expected: TokenTree
) -> TokenStream
  match take(stream, expected)
  | Ok(_), stream -> take_all(stream, expected)
  | Err(_), stream -> stream

pub fn expect(
  stream: TokenStream
  expected: TokenTree
) -> Result<Span * TokenStream, Diagnostic>
  let token, span = peek(stream)
  match token == expected
  | true  -> Ok((span, advance(stream, 1)))
  | false ->
    diagnostic:new(
      severity: Error
      code:     "E0003"
      message:  f"expected {format(expected)}, found {format(token)}"
    )
    |> diagnostic:add_label("here", span)
    |> Err

pub fn digits()
  []
  |> list:append("0")
  |> list:append("1")
  |> list:append("2")
  |> list:append("3")
  |> list:append("4")
  |> list:append("5")
  |> list:append("6")
  |> list:append("7")
  |> list:append("8")
  |> list:append("9")
  |> list:append("a")
  |> list:append("b")
  |> list:append("c")
  |> list:append("d")
  |> list:append("e")
  |> list:append("f")

pub fn keywords()
  dict:new()
  |> dict:insert("as", Keyword:As)
  |> dict:insert("assert", Keyword:Assert)
  |> dict:insert("bool", Keyword:Bool)
  |> dict:insert("f32", Keyword:F32)
  |> dict:insert("f64", Keyword:F64)
  |> dict:insert("false", Keyword:False)
  |> dict:insert("fn", Keyword:Fn)
  |> dict:insert("i16", Keyword:I16)
  |> dict:insert("i32", Keyword:I32)
  |> dict:insert("i64", Keyword:I64)
  |> dict:insert("i8", Keyword:I8)
  |> dict:insert("if", Keyword:If)
  |> dict:insert("import", Keyword:Import)
  |> dict:insert("int", Keyword:Int)
  |> dict:insert("let", Keyword:Let)
  |> dict:insert("match", Keyword:Match)
  |> dict:insert("mut", Keyword:Mut)
  |> dict:insert("panic", Keyword:Panic)
  |> dict:insert("pub", Keyword:Pub)
  |> dict:insert("pure", Keyword:Pure)
  |> dict:insert("return", Keyword:Return)
  |> dict:insert("str", Keyword:Str)
  |> dict:insert("todo", Keyword:Todo)
  |> dict:insert("true", Keyword:True)
  |> dict:insert("type", Keyword:Type)
  |> dict:insert("u16", Keyword:U16)
  |> dict:insert("u32", Keyword:U32)
  |> dict:insert("u64", Keyword:U64)
  |> dict:insert("u8", Keyword:U8)
  |> dict:insert("unreachable", Keyword:Unreachable)
  |> dict:insert("void", Keyword:Void)

pub fn puncts()
  dict:new()
  |> dict:insert("..", Punct:DotDot)
  |> dict:insert("==", Punct:EqEq)
  |> dict:insert("!=", Punct:NotEq)
  |> dict:insert("<=", Punct:LtEq)
  |> dict:insert(">=", Punct:GtEq)
  |> dict:insert("&&", Punct:AndAnd)
  |> dict:insert("||", Punct:OrOr)
  |> dict:insert("|>", Punct:OrGt)
  |> dict:insert("->", Punct:Arrow)
  |> dict:insert(":", Punct:Colon)
  |> dict:insert(";", Punct:Semi)
  |> dict:insert(".", Punct:Dot)
  |> dict:insert(",", Punct:Comma)
  |> dict:insert("=", Punct:Eq)
  |> dict:insert("!", Punct:Not)
  |> dict:insert("&", Punct:And)
  |> dict:insert("|", Punct:Or)
  |> dict:insert("<", Punct:Lt)
  |> dict:insert(">", Punct:Gt)
  |> dict:insert("+", Punct:Plus)
  |> dict:insert("-", Punct:Minus)
  |> dict:insert("*", Punct:Star)
  |> dict:insert("/", Punct:Slash)
  |> dict:insert("%", Punct:Percent)
  |> dict:insert("'", Punct:Quote)
  |> dict:insert("_", Punct:Under)
  |> dict:insert("#", Punct:Pound)
  |> dict:insert("?", Punct:Question)

pub fn debug(stream: parse:token:TokenStream) -> void
  debug_rec(stream, 0, "token")

fn debug_rec(stream: parse:token:TokenStream, acc: int, prefix: str) -> void
  let token, span = peek_nth(stream, acc)
  match token
  | Group(_, group_stream) ->
        debug_rec(group_stream, 0, f"{prefix} {format(token)}")
        debug_rec(stream, acc + 1, prefix)
  | token ->
    f"{prefix} {acc}: {format(token)}" |> std:io:println
    debug_rec(stream, acc + 1, prefix)
  | Eof -> void
